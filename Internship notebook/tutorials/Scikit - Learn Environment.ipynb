{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Why Scikit-Learn for machine learning?**\n",
    "\n",
    "Scikit-Learn, also known as sklearn, is Python’s premier general-purpose machine learning library. While you’ll find other packages that do better at certain tasks, Scikit-Learn’s versatility makes it the best starting place for most ML problems.\n",
    "\n",
    "It’s also a fantastic library for beginners because it offers a high-level interface for many tasks (e.g. preprocessing data, cross-validation, etc.). This allows you to better practice the entire machine learning workflow and understand the big picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn Tutorial Contents\n",
    "Here are the steps for building your first random forest model using Scikit-Learn:\n",
    "\n",
    "1. Set up your environment.\n",
    "2. Import libraries and modules.\n",
    "3. Load red wine data.\n",
    "4. Split data into training and test sets.\n",
    "5. Declare data preprocessing steps.\n",
    "6. Declare hyperparameters to tune.\n",
    "7. Tune model using cross-validation pipeline.\n",
    "8. Refit on the entire training set.\n",
    "9. Evaluate model pipeline on test data.\n",
    "10. Save model for further use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Set up your environment.\n",
    "\n",
    "Make sure the following are installed on your computer:\n",
    "\n",
    "* Python 2.7+ or Python 3\n",
    "* NumPy\n",
    "* Pandas\n",
    "* Scikit-Learn (a.k.a. sklearn)\n",
    "\n",
    "I strongly recommend installing Python through Anaconda. It comes with all of the above packages already installed.\n",
    "\n",
    "If you need to update any of the packages, it's as easy as typing  _$ conda update <package>_ from your command line program (Terminal in Mac).\n",
    "\n",
    "You can confirm Scikit-Learn was installed properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22.1\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print (sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Import libraries and modules.\n",
    "To begin, let's import numpy, which provides support for more efficient numerical computation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import numpy as np\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll import Pandas, a convenient library that supports dataframes . Pandas is technically optional because Scikit-Learn can handle numerical matrices directly, but it'll make our lives easier:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import pandas as pd\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to start importing functions for machine learning. The first one will be the train_test_split() function from the model_selection module. As its name implies, this module contains many utilities that will help us choose between models.\n",
    "\n",
    "```\n",
    "from sklearn.model_selection import train_test_split\n",
    "```\n",
    "\n",
    "Next, we'll import the entire preprocessing module. This contains utilities for scaling, transforming, and wrangling data.\n",
    "\n",
    "```\n",
    "from sklearn import preprocessing\n",
    "```\n",
    "\n",
    "Next, let's import the families of models we'll need\n",
    "\n",
    "**What's the difference between model \"families\" and actual models?**\n",
    "\n",
    "A \"family\" of models are broad types of models, such as random forests, SVM's, linear regression models, etc. Within each family of models, you'll get an actual model after you fit and tune its parameters to the data.\n",
    "\n",
    "*Tip: Don't worry too much about this for now... It will make more sense once we get to Step 7.\n",
    "\n",
    "We can import the random forest family like so:\n",
    "\n",
    "```\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "```\n",
    "\n",
    "For the scope of this tutorial, we'll only focus on training a random forest and tuning its parameters. We'll have another detailed tutorial for how to choose between model families.\n",
    "\n",
    "For now, let's move on to importing the tools to help us perform cross-validation.\n",
    "\n",
    "```\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "```\n",
    "Next, let's import some metrics we can use to evaluate our model performance later.\n",
    "\n",
    "```\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "```\n",
    "And finally, we'll import a way to persist our model for future use.\n",
    "\n",
    "```\n",
    "from sklearn.externals import joblib\n",
    "```\n",
    "\n",
    "Joblib is an alternative to Python's pickle package, and we'll use it because it's more efficient for storing large numpy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load red wine data.\n",
    "\n",
    "Alright, now we're ready to load our data set. The Pandas library that we imported is loaded with a whole suite of helpful import/output tools.\n",
    "\n",
    "You can read data from CSV, Excel, SQL, SAS, and many other data formats.\n",
    "\n",
    "The convenient tool we'll use today is the read_csv() function. Using this function, we can load any CSV file, even from a remote URL!\n",
    "\n",
    "Load wine data from remote URLPython\n",
    "```\n",
    "dataset_url = 'http://mlr.cs.umass.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "data = pd.read_csv(dataset_url)\n",
    "```\n",
    "\n",
    "Now let's take a look at the first 5 rows of data:\n",
    "```\n",
    "print data.head()\n",
    "```\n",
    "Upon further inspection, you will realise that the CSV file is actually using semicolons to separate the data. That's annoying, but easy to fix:\n",
    "\n",
    "\n",
    "```\n",
    "data = pd.read_csv(dataset_url, sep=';')\n",
    "```\n",
    "\n",
    "Now, let's take a look at the data.\n",
    "\n",
    "```\n",
    "print (data.shape)\n",
    "print (data.describe())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Split data into training and test sets.\n",
    "\n",
    "Splitting the data into training and test sets at the beginning of your modeling workflow is crucial for getting a realistic estimate of your model's performance.\n",
    "\n",
    "First, let's separate our target (y) features from our input (X) features:\n",
    "\n",
    "```\n",
    "y = data.quality\n",
    "X = data.drop('quality', axis=1)\n",
    "```\n",
    "This allows us to take advantage of Scikit-Learn's useful train_test_split function:\n",
    "\n",
    "```\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=123, \n",
    "                                                    stratify=y)\n",
    "```\n",
    "\n",
    "As you can see, we'll set aside 20% of the data as a test set for evaluating our model. We also set an arbitrary \"random state\" (a.k.a. seed) so that we can reproduce our results.\n",
    "\n",
    "Finally, it's good practice to stratify your sample by the target variable. This will ensure your training set looks similar to your test set, making your evaluation metrics more reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Declare data preprocessing steps.\n",
    "Remember, in Step 3, we made the mental note to standardize our features because they were on different scales.\n",
    "\n",
    "**WTF is standardization?**\n",
    "\n",
    "Standardization is the process of subtracting the means from each feature and then dividing by the feature standard deviations.\n",
    "\n",
    "Standardization is a common requirement for machine learning tasks. Many algorithms assume that all features are centered around zero and have approximately the same variance.\n",
    "\n",
    "Scikit-Learn makes data preprocessing a breeze. For example, it's pretty easy to simply scale a dataset:\n",
    "\n",
    "```\n",
    "X_train_scaled = preprocessing.scale(X_train)\n",
    "print X_trained_scaled\n",
    "```\n",
    "\n",
    "You can confirm that the scaled dataset is indeed centered at zero, with unit variance:\n",
    "\n",
    "```\n",
    "print X_train_scaled.mean(axis=0)\n",
    "print X_train_scaled.std(axis=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Declare hyperparameters to tune.\n",
    "\n",
    "Now it's time to consider the hyperparameters that we'll want to tune for our model.\n",
    "\n",
    "**WTF are hyperparameters?**\n",
    "\n",
    "There are two types of parameters we need to worry about: model parameters and hyperparameters. Models parameters can be learned directly from the data (i.e. regression coefficients), while hyperparameters cannot.\n",
    "\n",
    "Hyperparameters express \"higher-level\" structural information about the model, and they are typically set before training the model.\n",
    "\n",
    "As an example, let's take our random forest for regression:\n",
    "\n",
    "Within each decision tree, the computer can empirically decide where to create branches based on either mean-squared-error (MSE) or mean-absolute-error (MAE). Therefore, the actual branch locations are model parameters.\n",
    "\n",
    "However, the algorithm does not know which of the two criteria, MSE or MAE, that it should use. The algorithm also cannot decide how many trees to include in the forest. These are examples of hyperparameters that the user must set.\n",
    "\n",
    "We can list the tunable hyperparameters like so:\n",
    "\n",
    "```\n",
    "print pipeline.get_params()\n",
    "```\n",
    "\n",
    "Now, let's declare the hyperparameters we want to tune through cross-validation.\n",
    "\n",
    "```\n",
    "hyperparameters = { 'randomforestregressor__max_features' : ['auto', 'sqrt', 'log2'],\n",
    "                  'randomforestregressor__max_depth': [None, 5, 3, 1]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Tune model using a cross-validation pipeline.\n",
    "\n",
    "Now we're almost ready to dive into fitting our models. But first, we need to spend some time talking about cross-validation.\n",
    "\n",
    "This is one of the most important skills in all of machine learning because it helps you maximize model performance while reducing the chance of overfitting.\n",
    "\n",
    "**WTF is cross-validation (CV)?**\n",
    "\n",
    "Cross-validation is a process for reliably estimating the performance of a method for building a model by training and evaluating your model multiple times using the same method.\n",
    "\n",
    "Practically, that \"method\" is simply a set of hyperparameters in this context.\n",
    "\n",
    "These are the steps for CV:\n",
    "\n",
    "1. Split your data into k equal parts, or \"folds\" (typically k=10).\n",
    "2. Train your model on k-1 folds (e.g. the first 9 folds).\n",
    "3. Evaluate it on the remaining \"hold-out\" fold (e.g. the 10th fold).\n",
    "4. Perform steps (2) and (3) k times, each time holding out a different fold.\n",
    "5. Aggregate the performance across all k folds. This is your performance metric.\n",
    "\n",
    "\n",
    "Why is cross-validation important in machine learning?\n",
    "\n",
    "Let's say you want to train a random forest regressor. One of the hyperparameters you must tune is the maximum depth allowed for each decision tree in your forest.\n",
    "\n",
    "How can you decide?\n",
    "\n",
    "That's where cross-validation comes in. Using only your training set, you can use CV to evaluate different hyperparameters and estimate their effectiveness.\n",
    "\n",
    "This allows you to keep your test set \"untainted\" and save it for a true hold-out evaluation when you're finally ready to select a model.\n",
    "\n",
    "For example, you can use CV to tune a random forest model, a linear regression model, and a k-nearest neighbors model, using only the training set. Then, you still have the untainted test set to make your final selection between the model families!\n",
    "\n",
    "So WTF is a cross-validation \"pipeline?\"\n",
    "\n",
    "The best practice when performing CV is to include your data preprocessing steps inside the cross-validation loop. This prevents accidentally tainting your training folds with influential data from your test fold.\n",
    "\n",
    "Here's how the CV pipeline looks after including preprocessing steps:\n",
    "\n",
    "1. Split your data into k equal parts, or \"folds\" (typically k=10).\n",
    "2. Preprocess k-1 training folds.\n",
    "3. Train your model on the same k-1 folds.\n",
    "4. Preprocess the hold-out fold using the same transformations from step (2).\n",
    "5. Evaluate your model on the same hold-out fold.\n",
    "6. Perform steps (2) - (5) k times, each time holding out a different fold.\n",
    "7. Aggregate the performance across all k folds. This is your performance metric.\n",
    "\n",
    "Fortunately, Scikit-Learn makes it stupidly simple to set this up:\n",
    "\n",
    "```\n",
    "clf = GridSearchCV(pipeline, hyperparameters, cv=10)\n",
    " \n",
    "# Fit and tune model\n",
    "clf.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "Yes, it's really that easy. GridSearchCV essentially performs cross-validation across the entire \"grid\" (all possible permutations) of hyperparameters.\n",
    "\n",
    "It takes in your model (in this case, we're using a model pipeline), the hyperparameters you want to tune, and the number of folds to create.\n",
    "\n",
    "Obviously, there's a lot going on under the hood. We've included the pseudo-code above, and we'll cover writing cross-validation from scratch in a separate guide.\n",
    "\n",
    "Now, you can see the best set of parameters found using CV:\n",
    "\n",
    "```\n",
    "print clf.best_params_\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Refit on the entire training set.\n",
    "After you've tuned your hyperparameters appropriately using cross-validation, you can generally get a small performance improvement by refitting the model on the entire training set.\n",
    "\n",
    "Conveniently, GridSearchCV from sklearn will automatically refit the model with the best set of hyperparameters using the entire training set.\n",
    "\n",
    "This functionality is ON by default, but you can confirm it:\n",
    "\n",
    "```\n",
    "print clf.refit\n",
    "```\n",
    "Now, you can simply use the  clf object as your model when applying it to other sets of data. That's what we'll be doing in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Evaluate model pipeline on test data.\n",
    "Alright, we're in the home stretch!\n",
    "\n",
    "This step is really straightforward once you understand that the  clf object you used to tune the hyperparameters can also be used directly like a model object.\n",
    "\n",
    "Here's how to predict a new set of data:\n",
    "\n",
    "```\n",
    "y_pred = clf.predict(X_test)\n",
    "```\n",
    "\n",
    "Now we can use the metrics we imported earlier to evaluate our model performance.\n",
    "```\n",
    "print r2_score(y_test, y_pred)\n",
    "print mean_squared_error(y_test, y_pred)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Save model for future use.\n",
    "Great job completing this tutorial!\n",
    "\n",
    "You've done the hard part, and deserve another glass of wine. Maybe this time you can use your shiny new predictive model to select the bottle.\n",
    "\n",
    "But before you go, let's save your hard work so you can use the model in the future. It's really easy to do so:\n",
    "```\n",
    "joblib.dump(clf, 'rf_regressor.pkl')\n",
    "```\n",
    "\n",
    "And that's it. When you want to load the model again, simply use this function:\n",
    "\n",
    "```\n",
    "clf2 = joblib.load('rf_regressor.pkl')\n",
    "```\n",
    "\n",
    "Predict data set using loaded model...\n",
    "```\n",
    "clf2.predict(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The complete code, from start to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Import libraries and modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.externals import joblib \n",
    " \n",
    "# 3. Load red wine data.\n",
    "dataset_url = 'http://mlr.cs.umass.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "data = pd.read_csv(dataset_url, sep=';')\n",
    " \n",
    "# 4. Split data into training and test sets\n",
    "y = data.quality\n",
    "X = data.drop('quality', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=123, \n",
    "                                                    stratify=y)\n",
    " \n",
    "# 5. Declare data preprocessing steps\n",
    "pipeline = make_pipeline(preprocessing.StandardScaler(), \n",
    "                         RandomForestRegressor(n_estimators=100))\n",
    " \n",
    "# 6. Declare hyperparameters to tune\n",
    "hyperparameters = { 'randomforestregressor__max_features' : ['auto', 'sqrt', 'log2'],\n",
    "                  'randomforestregressor__max_depth': [None, 5, 3, 1]}\n",
    " \n",
    "# 7. Tune model using cross-validation pipeline\n",
    "clf = GridSearchCV(pipeline, hyperparameters, cv=10)\n",
    " \n",
    "clf.fit(X_train, y_train)\n",
    " \n",
    "# 8. Refit on the entire training set\n",
    "# No additional code needed if clf.refit == True (default is True)\n",
    " \n",
    "# 9. Evaluate model pipeline on test data\n",
    "pred = clf.predict(X_test)\n",
    "print r2_score(y_test, pred)\n",
    "print mean_squared_error(y_test, pred)\n",
    " \n",
    "# 10. Save model for future use\n",
    "joblib.dump(clf, 'rf_regressor.pkl')\n",
    "# To load: clf2 = joblib.load('rf_regressor.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
